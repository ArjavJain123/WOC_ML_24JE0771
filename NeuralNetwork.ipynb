{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMon3FiS6H5Z9LHWYFmkgkh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArjavJain123/WOC_ML_24JE0771/blob/Algorithms_Generalized/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lwo9RrcqEbE"
      },
      "outputs": [],
      "source": [
        "import numpy as np # type: ignore\n",
        "import pandas as pd # type: ignore\n",
        "import matplotlib.pyplot as plt # type: ignore\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "  def __init__(self, layer_list, act_funcs, alpha = 3e-2, batch_size=32, epochs=100, reg_param=None):\n",
        "    self.layer_list = layer_list\n",
        "    self.alpha = alpha\n",
        "    self.batch_size = batch_size\n",
        "    self.epochs = epochs\n",
        "    self.params = {}\n",
        "    self.cache = {}\n",
        "    self.reg_param = reg_param\n",
        "    self.grads = {}\n",
        "    self.act_funcs = act_funcs\n",
        "    self._init_params(log_it=True)\n",
        "\n",
        "  #Useful funcs:\n",
        "  def _relu(self, z):\n",
        "    return np.maximum(z,0)\n",
        "  def _softmax(self, z):\n",
        "    A = np.exp(z)/sum(np.exp(z))\n",
        "    return A\n",
        "  def _sigmoid(self, z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def _reluDer(self, z):\n",
        "    return (z >0).astype(int)\n",
        "  def _get_deltaZ_lastLayer(self,A, Y):\n",
        "    return A - Y\n",
        "  def _compute_cost(self,A, Y, cost_func,reg=True):\n",
        "    if 'soft' in cost_func:\n",
        "        m = Y.shape[1]\n",
        "        cost = -np.sum(Y * np.log(A)) / m\n",
        "        cost += self._add_reg_term(m) if reg else 0\n",
        "        return cost\n",
        "    elif 'sigm' in cost_func:\n",
        "        m = Y.shape[1]\n",
        "        cost = (-1 / m) * np.sum(np.multiply(Y, np.log(A)) + np.multiply(1 - Y, np.log(1 - A)))\n",
        "        cost += self._add_reg_term(m) if reg else 0\n",
        "        return cost\n",
        "    else:\n",
        "        raise Exception(\"Unsupported Cost function\")\n",
        "\n",
        "  def _add_reg_term(self,m):\n",
        "    L = len(self.layer_list) - 1\n",
        "    reg_cost = 0.0\n",
        "    for l in range(1,L+1):\n",
        "      reg_cost += np.sum(self.params[f\"W{l}\"]**2)\n",
        "    reg_cost = (reg_cost*self.reg_param)/(2*m)\n",
        "    return reg_cost\n",
        "  def _one_hot(self, Y):\n",
        "    uniq_labels = np.unique(Y)\n",
        "    C = len(uniq_labels)\n",
        "    m = Y.size\n",
        "    one_hot_Y = np.zeros((C, m))\n",
        "    label_to_index = {label: index for index, label in enumerate(uniq_labels)}\n",
        "    indices = [label_to_index[label] for label in Y]\n",
        "    one_hot_Y[indices, np.arange(m)] = 1\n",
        "    # for i in range(m):\n",
        "    #   one_hot_Y[Y[i], i] = 1\n",
        "    return one_hot_Y\n",
        "\n",
        "    #params create:\n",
        "  def _init_params(self,log_it=True):\n",
        "    L = len(self.layer_list)\n",
        "    for l in range(1,L):\n",
        "      self.params[f\"W{l}\"] = np.random.randn(self.layer_list[l], self.layer_list[l-1]) * np.sqrt(2 / self.layer_list[l-1]) #He method\n",
        "      self.params[f\"b{l}\"] = np.random.randn(self.layer_list[l], 1) * np.sqrt(2 / self.layer_list[l-1])\n",
        "    print(\"Parameters initialized: \", {key: val.shape for key, val in self.params.items()}) if log_it else None\n",
        "  def _fwd_prp(self, X):\n",
        "    self.cache[\"A0\"] = X\n",
        "    L = len(self.layer_list) - 1\n",
        "    for l in range(1, L+1):\n",
        "      Z = np.dot(self.params[f\"W{l}\"], self.cache[f\"A{l-1}\"]) + self.params[f\"b{l}\"]\n",
        "      self.cache[f\"Z{l}\"] = Z\n",
        "      if \"relu\" in self.act_funcs[l-1]:\n",
        "        self.cache[f\"A{l}\"] = self._relu(Z)\n",
        "      elif 'sig' in self.act_funcs[l-1]:\n",
        "        self.cache[f\"A{l}\"] = self._sigmoid(Z)\n",
        "      elif 'soft' in self.act_funcs[l-1]:\n",
        "        self.cache[f\"A{l}\"] = self._softmax(Z)\n",
        "      else:\n",
        "        raise Exception(\"Unsupported activation function\")\n",
        "    return self.cache[f\"A{L}\"]\n",
        "  def _update_grads(self, X, Y):\n",
        "    #No of layers\n",
        "    L = len(self.layer_list) - 1\n",
        "    m = Y.shape[1]\n",
        "    A_last = self.cache[f\"A{L}\"]\n",
        "    if self.act_funcs[L-1] == 'sigmoid' or \"softmax\":\n",
        "      dZ = self._get_deltaZ_lastLayer(A_last, Y)\n",
        "    self.grads[f\"dZ{L}\"] = dZ\n",
        "    self.grads[f\"dW{L}\"] = np.dot(dZ, self.cache[f\"A{L-1}\"].T) / m\n",
        "    self.grads[f\"db{L}\"] = np.sum(dZ, axis = 1, keepdims = True)\n",
        "    if self.reg_param is not None:\n",
        "      self.grads[f\"dW{L}\"] += (self.reg_param*self.params[f\"W{L}\"])/(2*m)\n",
        "    for l in reversed(range(1,L)):\n",
        "      dA = np.dot(self.params[f\"W{l+1}\"].T, dZ) #this is the dA of the current layer\n",
        "      Z = self.cache[f\"Z{l}\"]\n",
        "      A = self.cache[f\"A{l}\"]\n",
        "      activation = self.act_funcs[l-1] #when l = 1, it is the first hidden layer, correspondingly , activations[0] gives the act func of that layer\n",
        "      if activation == \"relu\":\n",
        "        dZ = dA * self._reluDer(Z)\n",
        "      elif activation == \"sigmoid\":\n",
        "        dZ = dA * (A * (1 - A))\n",
        "      else:\n",
        "        raise ValueError(f\"Unsupported activation function: {activation}\")\n",
        "\n",
        "      self.grads[f\"dZ{l}\"] = dZ\n",
        "      self.grads[f\"dW{l}\"] = (1/m)*np.dot(dZ, self.cache[f\"A{l-1}\"].T)\n",
        "      self.grads[f\"db{l}\"] = (1/m)*np.sum(dZ, axis = 1, keepdims = True)\n",
        "      if self.reg_param is not None:\n",
        "        self.grads[f\"dW{l}\"] += (self.reg_param*self.params[f\"W{l}\"])/(2*m)\n",
        "\n",
        "    # return self.grads\n",
        "  def _update_params(self):\n",
        "      L = len(self.layer_list) - 1\n",
        "      for l in range(1, L + 1):\n",
        "          self.params[f\"W{l}\"] -= self.alpha*self.grads[f\"dW{l}\"]\n",
        "          self.params[f\"b{l}\"] -= self.alpha*self.grads[f\"db{l}\"]\n",
        "\n",
        "\n",
        "  def train(self, X, Y, cost_func = 'soft',details=True,plot_costs=True):\n",
        "    m = X.shape[1]\n",
        "    if 'sigm' in self.act_funcs[-1]:\n",
        "      Y = Y.reshape(1,m)\n",
        "    else:\n",
        "      Y = self._one_hot(Y)\n",
        "    J_history_batches = []\n",
        "    J_history_entire = []\n",
        "    for epoch in range(1, self.epochs+1):\n",
        "        permutation = np.random.permutation(m)\n",
        "        X_shuffled = X[:, permutation]\n",
        "        Y_shuffled = Y[:, permutation]\n",
        "\n",
        "        batches = m // self.batch_size\n",
        "        for k in range(0, batches):\n",
        "            mini_batch_X = X_shuffled[:, k*self.batch_size:(k+1)*self.batch_size]\n",
        "            mini_batch_Y = Y_shuffled[:, k*self.batch_size:(k+1)*self.batch_size]\n",
        "\n",
        "            A_last = self._fwd_prp(mini_batch_X)\n",
        "            self._update_grads(mini_batch_X, mini_batch_Y)\n",
        "            self._update_params()\n",
        "            cost = self._compute_cost(A_last, mini_batch_Y, cost_func,reg=False)\n",
        "            J_history_batches.append(cost)\n",
        "            cost += self._add_reg_term(mini_batch_Y.shape[1]) if self.reg_param is not None else 0.0\n",
        "            # First I am appending normal cost, and then I am adding the reg term to it, for printing\n",
        "            if details:\n",
        "                print(f\"Epoch: {epoch:03d}, Batch: {k+1}/{batches}, Cost: {cost:.6f}\")\n",
        "        if m % batches != 0:\n",
        "            mini_batch_X = X_shuffled[:, batches*self.batch_size:m]\n",
        "            mini_batch_Y = Y_shuffled[:, batches*self.batch_size:m]\n",
        "\n",
        "            A_last = self._fwd_prp(mini_batch_X)\n",
        "            self._update_grads(mini_batch_X, mini_batch_Y)\n",
        "            self._update_params()\n",
        "            cost = self._compute_cost(A_last, mini_batch_Y, cost_func)\n",
        "            J_history_batches.append(cost)\n",
        "            cost += self._add_reg_term(mini_batch_Y.shape[1]) if self.reg_param is not None else 0\n",
        "            if details:\n",
        "                print(f\"Epoch: {epoch:03d}, Batch: last, Cost: {cost:.6f}\")\n",
        "        A_last = self._fwd_prp(X_shuffled)\n",
        "        cost = self._compute_cost(A_last, Y_shuffled, cost_func,reg=False)\n",
        "        J_history_entire.append(cost)\n",
        "        cost += self._add_reg_term(Y_shuffled.shape[1]) if self.reg_param is not None else 0\n",
        "        if 'sigm' in self.act_funcs[-1]:\n",
        "          predictions = self.predict_bin(X_shuffled)\n",
        "        else:\n",
        "          predictions = self.predict(X_shuffled)\n",
        "        true_Y = np.argmax(Y_shuffled, axis=0)\n",
        "        accuracy = self.get_accuracy(predictions, true_Y)\n",
        "        print(f\"Epoch: {epoch:03d}, Cost: {cost:.6f}, accuracy: {accuracy:.4f}\")\n",
        "    self._plotter(J_history_entire) if plot_costs else None\n",
        "    return J_history_batches, J_history_entire\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "      A_last = self._fwd_prp(X)\n",
        "      predictions = np.argmax(A_last, axis = 0)\n",
        "      return predictions\n",
        "  def get_accuracy(self, predictions, Y):\n",
        "      accuracy = 100* np.mean(predictions == Y)\n",
        "      return accuracy\n",
        "\n",
        "  def k_fold_cv(self, X, Y, k_folds, cost_func, details=True,plot_acc=True,plot_cost_vs_epoch=True,retrain=True):\n",
        "    \"\"\"Dont pass in standardized X in here\"\"\"\n",
        "    m = X.shape[1]\n",
        "    indices = np.random.permutation(m)\n",
        "    X_shuffled = X[:, indices]\n",
        "    if plot_cost_vs_epoch:\n",
        "      J_hist_list = []\n",
        "    # Y = self._one_hot(Y) dont use one hot here, as the train function takes the 1d Y and does the one hot there only\n",
        "    Y_shuffled = Y[indices]\n",
        "    fold_size = m//k_folds\n",
        "    training_accuracies = []\n",
        "    testing_accuracies = []\n",
        "    for fold in range(k_folds):\n",
        "      print(f\"Working on fold {fold+1}.....\")\n",
        "      test_start = fold*fold_size\n",
        "      test_end = (fold+1)*fold_size if fold < k_folds - 1 else m\n",
        "      self._init_params(log_it=False)\n",
        "      X_train = np.concatenate((X_shuffled[:, :test_start], X_shuffled[:, test_end:]), axis = 1)\n",
        "      Y_train = np.concatenate((Y_shuffled[:test_start], Y_shuffled[test_end:]))\n",
        "      X_test = X_shuffled[:,test_start:test_end]\n",
        "      Y_test = Y_shuffled[test_start:test_end]\n",
        "      X_train_mean = (np.mean(X_train, axis=1)).reshape((1,X_train_mean.size))\n",
        "      X_train_std = (np.std(X_train, axis = 1)).reshape((1,X_train_mean.size))\n",
        "      X_train = (X_train - X_train_mean)/X_train_std\n",
        "      X_test  = (X_test - X_train_mean)/X_train_std\n",
        "      useless,J_hist = self.train(X_train, Y_train, cost_func, details,plot_costs=False)\n",
        "      if plot_cost_vs_epoch:\n",
        "        J_hist_list.append(J_hist)\n",
        "      training_predictions = self.predict_bin(X_train) if 'sigm' in self.act_funcs[-1] else self.predict(X_train)\n",
        "      testing_predictions = self.predict_bin(X_test) if 'sigm' in self.act_funcs[-1] else self.predict(X_test)\n",
        "      # training_predictions = self.predict(X_train)\n",
        "      # testing_predictions = self.predict(X_test)\n",
        "      training_accuracy = self.get_accuracy(training_predictions, Y_train)\n",
        "      print(f\"The training accuracy for fold: {fold+1} is {training_accuracy:.4f}\")\n",
        "      testing_accuracy = self.get_accuracy(testing_predictions, Y_test)\n",
        "      print(f\"The testing accuracy for fold: {fold+1} is {testing_accuracy:.4f}\")\n",
        "      training_accuracies.append(training_accuracy)\n",
        "      testing_accuracies.append(testing_accuracy)\n",
        "      print(f\"Fold {fold+1} Completed!\")\n",
        "      print(f\"Starting fold {fold+2}\") if fold < k_folds - 1 else None\n",
        "    mean_of_training_accuracies = np.mean(training_accuracies)\n",
        "    mean_of_testing_accuracies = np.mean(testing_accuracies)\n",
        "    print(f\"Mean of training accuracies: {mean_of_training_accuracies:.4f}\")\n",
        "    print(f\"Mean of testing accuracies: {mean_of_testing_accuracies:.4f}\")\n",
        "    if plot_cost_vs_epoch:\n",
        "      for fold in range(k_folds):\n",
        "            plt.plot(np.arange(1,len(J_hist_list[fold])+1), J_hist_list[fold], c='r')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Cost')\n",
        "            plt.title(f\"Cost vs Epochs for fold {fold+1}\")\n",
        "            plt.show()\n",
        "    if retrain:\n",
        "      print(\"Retraining the model on the entire dataset....\")\n",
        "      self._init_params(log_it=True)\n",
        "      self.train(X, Y, cost_func, details,plot_costs=False)\n",
        "      print(\"Retraining complete.\")\n",
        "\n",
        "\n",
        "    self._plotter_for_CV(training_accuracies,testing_accuracies) if plot_acc else None\n",
        "\n",
        "    return training_accuracies, testing_accuracies, mean_of_training_accuracies, mean_of_testing_accuracies\n",
        "\n",
        "\n",
        "  def _plotter(self, J_history):\n",
        "    plt.plot(np.arange(1,len(J_history)+1), J_history, c='r')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Cost')\n",
        "    plt.title(\"Cost vs Epochs\")\n",
        "    plt.show()\n",
        "\n",
        "  def _plotter_for_CV(self, training_accuracies, testing_accuracies):\n",
        "    k_folds = len(training_accuracies)\n",
        "    plt.plot(np.arange(1,k_folds+1), training_accuracies, c='green',label='Training Accuracy')\n",
        "    plt.plot(np.arange(1,k_folds+1), testing_accuracies, c='blue',label='Testing Accuracy')\n",
        "    plt.xlabel(\"Folds\")\n",
        "    plt.ylabel(\"Accuracies\")\n",
        "    plt.title(\"Folds vs Accuracies\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  def predict_bin(self,X):\n",
        "      A_last = self._fwd_prp(X)\n",
        "      # predictions = np.zeros((X.shape[0],))\n",
        "      # for i in range(len(X.shape[0])):\n",
        "      #   predictions[i] = 1 if A_last[i] > 0.5 else 0\n",
        "      predictions = (A_last > 0.5).astype(int)\n",
        "      return predictions\n",
        "\n",
        "  def get_conf_mat(self, Y_act, Y_pred):\n",
        "    n_cls = len(np.unique(Y_act))\n",
        "    con_mat = np.zeros((n_cls, n_cls))\n",
        "    for indx, label in enumerate(Y_act):\n",
        "      con_mat[label, Y_pred[indx]] += 1\n",
        "\n",
        "    return con_mat\n",
        "\n",
        "  def print_con_mat(self, con_mat):\n",
        "    print(\"Confusion matrix with predictions on X axis and actual values on Y\")\n",
        "    classes= np.arange(con_mat.shape[0])\n",
        "    table = pd.DataFrame(con_mat, index=classes, columns=classes)\n",
        "    print(table)\n",
        "\n",
        "  def get_precision(self,con_mat):\n",
        "    precs = np.zeros((con_mat.shape[0],))\n",
        "    for i in range(con_mat.shape[0]):\n",
        "      precs[i] = con_mat[i,i]/np.sum(con_mat[:,i])\n",
        "\n",
        "    return precs\n",
        "\n",
        "  def get_recall(self, con_mat):\n",
        "    recs = np.zeros((con_mat.shape[0],))\n",
        "    for i in range(con_mat.shape[0]):\n",
        "      recs[i] = con_mat[i,i]/np.sum(con_mat[i,:])\n",
        "\n",
        "    return recs\n",
        "\n",
        "  def get_f1s(self, con_mat):\n",
        "    precs = self.get_precision(con_mat)\n",
        "    recs = self.get_recall(con_mat)\n",
        "    f1s = 2*recs*precs/(recs + precs)\n",
        "    return f1s\n",
        "\n",
        "  def classification_report(self, Y_act, Y_pred):\n",
        "    con_mat = self.get_conf_mat(Y_act, Y_pred)\n",
        "    precs = self.get_precision(con_mat)\n",
        "    recs = self.get_recall(con_mat)\n",
        "    f1s = self.get_f1s(con_mat)\n",
        "    supports = con_mat.sum(axis=1)\n",
        "    classes= np.arange(con_mat.shape[0])\n",
        "    reports = []\n",
        "    for i in range(con_mat.shape[0]):\n",
        "      reports.append([precs[i], recs[i], f1s[i], supports[i]])\n",
        "    table1 = pd.DataFrame(reports,index=classes, columns=['precision','recall','f1-score','support'])\n",
        "    # print(table1)\n",
        "    wted_prec = np.average(precs, weights=supports)\n",
        "    wted_rec = np.average(recs, weights=supports)\n",
        "    wted_f1 = np.average(f1s, weights=supports)\n",
        "    tot_support = np.sum(supports)\n",
        "    wted_data = [[wted_prec,wted_rec,wted_f1,tot_support]]\n",
        "    table2 = pd.DataFrame(wted_data,index=['Average (weighted)'],columns=['precision', 'recall', 'f1-score', 'support'])\n",
        "    # print('\\n\\n', table2)\n",
        "    table = pd.concat([table1, table2])\n",
        "    print(table)\n",
        "    print(f\"Accuracy: {self.get_accuracy(Y_pred, Y_act)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u4Y90MAWquPv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}